import streamlit as st
import numpy as np
import plotly.graph_objects as go
from scipy import stats

def render():
    st.markdown("""
    <div class='topic-header'>
        <h1>âš–ï¸ Hypothesis Testing</h1>
        <p>A complete guide â€” framework, p-values, Z-tests, t-tests, errors, and decision making.</p>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("<div class='section-card'><div class='section-label label-intro'>ğŸ“– Introduction</div>", unsafe_allow_html=True)
    st.markdown("""
**Hypothesis testing** is the most widely used procedure in inferential statistics. It provides a **formal, repeatable framework** for deciding whether sample data provides strong enough evidence to reject a claim about a population.

**Real-world use cases:**
- Is a new drug more effective than the existing one?
- Has the average delivery time changed after a process change?
- Is the defect rate below the acceptable threshold?
- Do male and female customers spend different amounts?

**The Big Idea:** We assume the status quo (null hypothesis) is true, then ask: *"How unlikely is the observed data under this assumption?"* If the data is very unlikely, we reject the status quo.
    """)
    st.markdown("</div>", unsafe_allow_html=True)

    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“‹ Framework & Steps",
        "âš ï¸ Type I & II Errors",
        "ğŸ“Š P-Values (Detailed)",
        "ğŸ”” Z-Tests (Ïƒ Known)",
        "ğŸ“ˆ t-Tests (Ïƒ Unknown)"
    ])

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 1: Framework & Steps
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab1:
        st.markdown("<div class='section-card'><div class='section-label label-concept'>ğŸ’¡ The 5-Step Hypothesis Testing Procedure</div>", unsafe_allow_html=True)

        st.markdown("### Step 1: State the Hypotheses")
        col1, col2 = st.columns(2)
        with col1:
            st.latex(r"H_0: \text{Null Hypothesis (status quo, claim to test)}")
            st.latex(r"H_a: \text{Alternative Hypothesis (what we want to show)}")
            st.markdown("""
**Rules:**
- Hâ‚€ **always** contains the equality (=, â‰¤, or â‰¥)
- Hâ‚ is the **research hypothesis** â€” what you're trying to prove
- The burden of proof is on Hâ‚ (we assume Hâ‚€ until evidence says otherwise)
            """)
        with col2:
            st.markdown("#### Three Types of Tests")
            st.markdown("""
| Test Name | Hâ‚€ | Hâ‚ | Rejection Region |
|-----------|----|----|-----------------|
| **Two-tailed** | Î¼ = Î¼â‚€ | Î¼ â‰  Î¼â‚€ | Both tails |
| **Right-tailed (upper)** | Î¼ â‰¤ Î¼â‚€ | Î¼ > Î¼â‚€ | Right tail |
| **Left-tailed (lower)** | Î¼ â‰¥ Î¼â‚€ | Î¼ < Î¼â‚€ | Left tail |
            """)
            st.info("ğŸ’¡ **Tip:** The direction of Hâ‚ (>, <, or â‰ ) is determined by the research question, NOT by the data.")

        st.markdown("### Step 2: Choose the Significance Level (Î±)")
        st.markdown("""
- **Î±** = maximum acceptable probability of rejecting Hâ‚€ when it is actually true (Type I error)
- Common choices: **Î± = 0.05** (most common), 0.01 (strict), 0.10 (lenient)
- Must be chosen **before** looking at data
        """)

        st.markdown("### Step 3: Compute the Test Statistic")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**Z-test (Ïƒ known):**")
            st.latex(r"z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}}")
        with col2:
            st.markdown("**t-test (Ïƒ unknown):**")
            st.latex(r"t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}, \quad df = n-1")

        st.markdown("### Step 4: Determine the P-Value (or compare with critical value)")
        st.markdown("*See the dedicated P-Value tab for full details â†’*")

        st.markdown("### Step 5: Make the Decision")
        st.markdown("""
| If... | Decision | Wording |
|-------|----------|---------|
| **p-value < Î±** | **Reject Hâ‚€** | "There is sufficient evidence at the Î± level to conclude that Hâ‚" |
| **p-value â‰¥ Î±** | **Fail to reject Hâ‚€** | "There is insufficient evidence at the Î± level to conclude that Hâ‚" |
        """)
        st.warning("âš ï¸ **NEVER say 'Accept Hâ‚€'**. We either reject or fail to reject. Failing to reject does not prove Hâ‚€ is true â€” it simply means we don't have enough evidence against it.")

        st.markdown("---")
        st.markdown("#### ğŸ“Œ Two Equivalent Decision Methods")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("##### Method 1: P-Value Approach")
            st.markdown("""
1. Compute test statistic
2. Compute p-value
3. If **p-value < Î±** â†’ Reject Hâ‚€
4. Most widely used (provides more information)
            """)
        with col2:
            st.markdown("##### Method 2: Critical Value Approach")
            st.markdown("""
1. Find the critical value(s) from z/t table
2. Compute test statistic
3. If test stat falls in **rejection region** â†’ Reject Hâ‚€
4. Traditional textbook approach
            """)
        st.markdown("Both methods **always** give the same decision.")
        st.markdown("</div>", unsafe_allow_html=True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 2: Type I & II Errors
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab2:
        st.markdown("<div class='section-card'><div class='section-label label-concept'>ğŸ’¡ Type I and Type II Errors</div>", unsafe_allow_html=True)

        st.markdown("#### Decision Matrix")
        st.markdown("""
|  | **Hâ‚€ is actually TRUE** | **Hâ‚€ is actually FALSE** |
|--|------------------------|-------------------------|
| **Reject Hâ‚€** | âŒ **Type I Error** (Î±)<br>False Positive | âœ… **Correct** (Power = 1âˆ’Î²)<br>True Positive |
| **Fail to reject Hâ‚€** | âœ… **Correct** (1âˆ’Î±)<br>True Negative | âŒ **Type II Error** (Î²)<br>False Negative |
        """)

        st.markdown("---")

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### âŒ Type I Error (False Positive)")
            st.latex(r"\alpha = P(\text{Reject } H_0 \mid H_0 \text{ is true})")
            st.markdown("""
**Real-world analogy:** Convicting an innocent person.

**Examples:**
- Concluding a drug works when it doesn't â†’ patients take an ineffective drug
- Declaring a manufacturing process out of control when it's fine â†’ unnecessary shutdowns
- Finding a "significant" marketing effect that's actually random noise â†’ wasted budget

**Controlled by:** The researcher sets Î± before the test.
            """)

        with col2:
            st.markdown("### âŒ Type II Error (False Negative)")
            st.latex(r"\beta = P(\text{Fail to reject } H_0 \mid H_0 \text{ is false})")
            st.markdown("""
**Real-world analogy:** Letting a guilty person go free.

**Examples:**
- Missing a real drug effect â†’ patients denied an effective treatment
- Not detecting an actual shift in quality â†’ defective products shipped
- Missing a real difference in customer preferences â†’ missed opportunity

**Harder to control.** Depends on: sample size n, effect size, Ïƒ, and Î±.
            """)

        st.markdown("---")
        st.markdown("### âš¡ Power of a Test")
        st.latex(r"\text{Power} = 1 - \beta = P(\text{Reject } H_0 \mid H_0 \text{ is false})")
        st.markdown("""
Power is the probability of **correctly detecting a real effect**. Higher is better (target: 0.80+).

#### What Increases Power?
| Factor | Direction | Why |
|--------|-----------|-----|
| â†‘ Sample size n | â†‘â†‘ Power | SE decreases â†’ test more sensitive |
| â†‘ Effect size (distance from Î¼â‚€) | â†‘ Power | Bigger effect easier to detect |
| â†‘ Î± (e.g. 0.05 â†’ 0.10) | â†‘ Power | Wider rejection region (but more Type I risk) |
| â†“ Ïƒ (population spread) | â†‘ Power | Less noise â†’ clearer signal |

#### The Î±-Î² Trade-off
Decreasing Î± (being more strict about Type I errors) **increases Î²** (makes Type II errors more likely). You cannot minimise both simultaneously without increasing n.
        """)
        st.markdown("</div>", unsafe_allow_html=True)

        st.markdown("<div class='section-card'><div class='section-label label-solved'>âœ… Solved Problem</div>", unsafe_allow_html=True)
        st.markdown("<span class='prob-badge'>Problem â€” Error Classification</span>", unsafe_allow_html=True)
        st.markdown("""
**Q:** A pharmaceutical company tests Hâ‚€: Î¼ = 50 (old drug effect) vs Hâ‚: Î¼ > 50 (new drug is better) at Î± = 0.05.

Classify the following scenarios:
1. The new drug actually works (Î¼ = 55) but the test fails to reject Hâ‚€.
2. The old drug is fine (Î¼ = 50) but the test rejects Hâ‚€ anyway.
3. The new drug works (Î¼ = 55) and the test correctly rejects Hâ‚€.

**Solution:**
1. **Type II Error (Î²)** â€” a real effect was missed (false negative)
2. **Type I Error (Î± = 0.05)** â€” a non-existent effect was declared significant (false positive)
3. **Correct decision (Power = 1âˆ’Î²)** â€” the test correctly detected the real effect
        """)
        st.markdown("</div>", unsafe_allow_html=True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 3: P-Values (Detailed)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab3:
        st.markdown("<div class='section-card'><div class='section-label label-concept'>ğŸ’¡ What is a P-Value?</div>", unsafe_allow_html=True)

        st.markdown("#### Formal Definition")
        st.latex(r"p\text{-value} = P(\text{observing data as extreme or more extreme than sample} \mid H_0 \text{ is true})")

        st.markdown("""
**In plain English:** The p-value is the probability of getting results **at least as surprising** as what we observed, **assuming the null hypothesis is true**.

**Key points:**
- It is **NOT** P(Hâ‚€ is true) â€” this is a common misconception!
- It is **NOT** the probability of making an error
- Smaller p-value = stronger evidence against Hâ‚€
- It is a measure of **compatibility** between the data and Hâ‚€
        """)

        st.markdown("---")
        st.markdown("#### How P-Values Are Computed")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.markdown("##### Right-tailed (Hâ‚: Î¼ > Î¼â‚€)")
            st.latex(r"p = P(Z \geq z_{obs})")
            st.latex(r"= 1 - \Phi(z_{obs})")
            st.caption("Area to the RIGHT of test stat")
        with col2:
            st.markdown("##### Left-tailed (Hâ‚: Î¼ < Î¼â‚€)")
            st.latex(r"p = P(Z \leq z_{obs})")
            st.latex(r"= \Phi(z_{obs})")
            st.caption("Area to the LEFT of test stat")
        with col3:
            st.markdown("##### Two-tailed (Hâ‚: Î¼ â‰  Î¼â‚€)")
            st.latex(r"p = 2 \times P(Z \geq |z_{obs}|)")
            st.latex(r"= 2 \times [1 - \Phi(|z_{obs}|)]")
            st.caption("Double the one-tail probability")

        st.markdown("---")
        st.markdown("#### ğŸ“Š Interpreting P-Values â€” Strength of Evidence")
        st.markdown("""
| P-value Range | Interpretation | Star Notation |
|--------------|---------------|---------------|
| p > 0.10 | No evidence against Hâ‚€ | (not significant) |
| 0.05 < p â‰¤ 0.10 | Weak evidence against Hâ‚€ | â€¢ (marginal) |
| 0.01 < p â‰¤ 0.05 | Evidence against Hâ‚€ | * (significant) |
| 0.001 < p â‰¤ 0.01 | Strong evidence against Hâ‚€ | ** (highly significant) |
| p â‰¤ 0.001 | Very strong evidence against Hâ‚€ | *** (very highly significant) |
        """)

        st.markdown("---")
        st.markdown("#### ğŸ”´ Common P-Value Misconceptions")
        col1, col2 = st.columns(2)
        with col1:
            st.error("""
**âŒ WRONG: "p = 0.03 means there's a 3% chance Hâ‚€ is true."**

p-value assumes Hâ‚€ is true and asks about the data, not the other way around. To get P(Hâ‚€ is true | data), you need Bayes' theorem.
            """)
            st.error("""
**âŒ WRONG: "p = 0.06 means the study failed."**

p = 0.06 is *marginal* â€” it suggests some evidence but doesn't meet Î± = 0.05 cutoff. It does NOT mean there is no effect.
            """)
        with col2:
            st.error("""
**âŒ WRONG: "p = 0.001 means the effect is large and important."**

A tiny p-value means the evidence against Hâ‚€ is strong, but the effect could be practically **trivial**. With n = 100,000, even a 0.1-unit difference can produce p < 0.001. Always report **effect size** alongside p-value.
            """)
            st.error("""
**âŒ WRONG: "Not significant means there's no difference."**

"Fail to reject Hâ‚€" â‰  "Hâ‚€ is true". The study might simply have been underpowered (too small n) to detect a real effect.
            """)

        st.markdown("---")
        st.markdown("#### ğŸ§® Interactive P-Value Calculator")
        col1, col2 = st.columns(2)
        with col1:
            dist_type = st.radio("Distribution:", ["Z (Standard Normal)", "t (Student's t)"], key="pv_dist")
            test_dir = st.radio("Tail:", ["Right (>)", "Left (<)", "Two-tailed (â‰ )"], key="pv_tail")
            stat_val = st.number_input("Test statistic value:", value=2.10, step=0.01, key="pv_stat")
            if "t" in dist_type:
                df_pv = st.number_input("Degrees of freedom:", value=20, min_value=1, key="pv_df")

        with col2:
            if "Z" in dist_type:
                if "Right" in test_dir:
                    pv = 1 - stats.norm.cdf(stat_val)
                elif "Left" in test_dir:
                    pv = stats.norm.cdf(stat_val)
                else:
                    pv = 2 * (1 - stats.norm.cdf(abs(stat_val)))
            else:
                if "Right" in test_dir:
                    pv = 1 - stats.t.cdf(stat_val, df_pv)
                elif "Left" in test_dir:
                    pv = stats.t.cdf(stat_val, df_pv)
                else:
                    pv = 2 * (1 - stats.t.cdf(abs(stat_val), df_pv))

            st.metric("P-value", f"{pv:.6f}")
            if pv <= 0.001:
                stars = "*** (Very highly significant)"
            elif pv <= 0.01:
                stars = "** (Highly significant)"
            elif pv <= 0.05:
                stars = "* (Significant)"
            elif pv <= 0.10:
                stars = "â€¢ (Marginal)"
            else:
                stars = "(Not significant)"
            st.info(f"**Evidence level:** {stars}")

            for a_check in [0.01, 0.05, 0.10]:
                if pv < a_check:
                    st.success(f"Reject Hâ‚€ at Î± = {a_check}")
                else:
                    st.warning(f"Fail to reject at Î± = {a_check}")

        # P-value visual
        x_range = np.linspace(-4, 4, 300)
        if "Z" in dist_type:
            y_curve = stats.norm.pdf(x_range)
        else:
            y_curve = stats.t.pdf(x_range, df_pv)

        fig = go.Figure()
        fig.add_trace(go.Scatter(x=x_range, y=y_curve, mode='lines',
                                  line=dict(color='#4f46e5', width=2), name='Distribution'))
        # Shade p-value region
        if "Right" in test_dir:
            x_shade = x_range[x_range >= stat_val]
            y_shade = stats.norm.pdf(x_shade) if "Z" in dist_type else stats.t.pdf(x_shade, df_pv)
            fig.add_trace(go.Scatter(x=x_shade, y=y_shade, fill='tozeroy',
                                      fillcolor='rgba(220,38,38,0.3)', line=dict(width=0), name=f'p = {pv:.4f}'))
        elif "Left" in test_dir:
            x_shade = x_range[x_range <= stat_val]
            y_shade = stats.norm.pdf(x_shade) if "Z" in dist_type else stats.t.pdf(x_shade, df_pv)
            fig.add_trace(go.Scatter(x=x_shade, y=y_shade, fill='tozeroy',
                                      fillcolor='rgba(220,38,38,0.3)', line=dict(width=0), name=f'p = {pv:.4f}'))
        else:
            for sv in [abs(stat_val), -abs(stat_val)]:
                if sv > 0:
                    xs = x_range[x_range >= sv]
                else:
                    xs = x_range[x_range <= sv]
                ys = stats.norm.pdf(xs) if "Z" in dist_type else stats.t.pdf(xs, df_pv)
                fig.add_trace(go.Scatter(x=xs, y=ys, fill='tozeroy',
                                          fillcolor='rgba(220,38,38,0.3)', line=dict(width=0), showlegend=False))
        fig.add_vline(x=stat_val, line_dash="dash", line_color="#dc2626",
                      annotation_text=f"stat={stat_val:.2f}")
        fig.update_layout(title=f"P-value = {pv:.6f} (shaded area)", paper_bgcolor='#ffffff',
                          plot_bgcolor='#f8fafc', font_color='#111111', height=300,
                          xaxis=dict(gridcolor='#e2e8f0'), yaxis=dict(gridcolor='#e2e8f0'))
        st.plotly_chart(fig, use_container_width=True)
        st.markdown("</div>", unsafe_allow_html=True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 4: Z-Tests
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab4:
        st.markdown("<div class='section-card'><div class='section-label label-concept'>ğŸ’¡ Z-Tests (Ïƒ Known)</div>", unsafe_allow_html=True)
        st.markdown("#### When to Use")
        st.markdown("""
- Population standard deviation Ïƒ is **known**
- Sample size is large (n â‰¥ 30) â€” even if Ïƒ is estimated, Z is acceptable
- Population is approximately Normal (or n is large enough for CLT)
        """)
        st.markdown("#### Test Statistic")
        st.latex(r"z = \frac{\bar{x} - \mu_0}{\sigma/\sqrt{n}}")
        st.markdown("#### Critical Values")
        st.markdown(r"""
| Î± | One-tailed z* | Two-tailed Â±z* |
|---|--------------|----------------|
| 0.10 | 1.282 | Â±1.645 |
| 0.05 | 1.645 | Â±1.960 |
| 0.01 | 2.326 | Â±2.576 |
        """)
        st.markdown("</div>", unsafe_allow_html=True)

        st.markdown("<div class='section-card'><div class='section-label label-solved'>ğŸ§® Interactive Z-Test Calculator</div>", unsafe_allow_html=True)
        col1, col2 = st.columns(2)
        with col1:
            mu_0 = st.number_input("Î¼â‚€ (hypothesised mean):", value=100.0, step=1.0, key="zt_mu")
            xbar = st.number_input("xÌ„ (sample mean):", value=103.5, step=0.5, key="zt_xbar")
            sigma = st.number_input("Ïƒ (population SD):", value=15.0, min_value=0.01, step=0.5, key="zt_sig")
            n_test = st.number_input("n (sample size):", value=36, min_value=2, key="zt_n")
            alpha = st.selectbox("Î±:", [0.01, 0.05, 0.10], index=1, key="zt_alpha")
            test_type = st.radio("Hâ‚:", ["Î¼ > Î¼â‚€ (upper)", "Î¼ < Î¼â‚€ (lower)", "Î¼ â‰  Î¼â‚€ (two-tailed)"], key="zt_type")
        with col2:
            se = sigma / np.sqrt(n_test)
            z_stat = (xbar - mu_0) / se
            if "upper" in test_type:
                p_val = 1 - stats.norm.cdf(z_stat)
                z_crit = stats.norm.ppf(1 - alpha)
                rej = z_stat > z_crit
            elif "lower" in test_type:
                p_val = stats.norm.cdf(z_stat)
                z_crit = stats.norm.ppf(alpha)
                rej = z_stat < z_crit
            else:
                p_val = 2 * (1 - stats.norm.cdf(abs(z_stat)))
                z_crit = stats.norm.ppf(1 - alpha/2)
                rej = abs(z_stat) > z_crit
            st.metric("SE = Ïƒ/âˆšn", f"{se:.4f}")
            st.metric("z-statistic", f"{z_stat:.4f}")
            st.metric("p-value", f"{p_val:.6f}")
            st.metric("Critical z*", f"Â±{z_crit:.3f}" if "two" in test_type else f"{z_crit:.3f}")
            if rej:
                st.error(f"**REJECT Hâ‚€** at Î±={alpha} (p={p_val:.4f} < {alpha})")
            else:
                st.success(f"**Fail to reject Hâ‚€** at Î±={alpha} (p={p_val:.4f} â‰¥ {alpha})")
        st.markdown("</div>", unsafe_allow_html=True)

        st.markdown("<div class='section-card'><div class='section-label label-solved'>âœ… Solved Problems</div>", unsafe_allow_html=True)
        st.markdown("<span class='prob-badge'>Problem 1 â€” Upper Tail Z-Test</span>", unsafe_allow_html=True)
        st.markdown("""
**Q:** A manufacturer claims bulb lifetime Î¼ = 1000 hours (Ïƒ = 120). Sample of 50: xÌ„ = 1035. At Î± = 0.05, is Î¼ > 1000?

**Solution:** Hâ‚€: Î¼ â‰¤ 1000, Hâ‚: Î¼ > 1000 (right-tailed)
        """)
        st.latex(r"z = \frac{1035 - 1000}{120/\sqrt{50}} = \frac{35}{16.97} = 2.063")
        st.latex(r"p\text{-value} = 1-\Phi(2.063) = 0.0196 < 0.05 \implies \text{Reject } H_0")
        st.markdown("**Conclusion:** Sufficient evidence that mean lifetime exceeds 1000 hours.")
        st.divider()

        st.markdown("<span class='prob-badge'>Problem 2 â€” Lower Tail Z-Test</span>", unsafe_allow_html=True)
        st.markdown("""
**Q:** Fill specification Î¼ â‰¥ 500ml (Ïƒ = 8). n = 40, xÌ„ = 497.5. At Î± = 0.01, is fill below standard?

**Solution:** Hâ‚€: Î¼ â‰¥ 500, Hâ‚: Î¼ < 500 (left-tailed)
        """)
        st.latex(r"z = \frac{497.5-500}{8/\sqrt{40}} = \frac{-2.5}{1.265} = -1.976")
        st.latex(r"p = \Phi(-1.976) = 0.0241 > 0.01 \implies \text{Fail to reject } H_0")
        st.markdown("**Conclusion at Î±=0.01:** Insufficient evidence of under-filling. *(Note: Would reject at Î±=0.05 since 0.0241 < 0.05.)*")
        st.divider()

        st.markdown("<span class='prob-badge'>Problem 3 â€” Two-Tailed Z-Test</span>", unsafe_allow_html=True)
        st.markdown("""
**Q:** A cereal box should weigh Î¼ = 400g (Ïƒ = 12). n = 64, xÌ„ = 397.9. At Î± = 0.05, has the mean changed?

**Solution:** Hâ‚€: Î¼ = 400, Hâ‚: Î¼ â‰  400 (two-tailed)
        """)
        st.latex(r"z = \frac{397.9-400}{12/\sqrt{64}} = \frac{-2.1}{1.5} = -1.40")
        st.latex(r"p = 2\times\Phi(-1.40) = 2\times0.0808 = 0.1616 > 0.05 \implies \text{Fail to reject } H_0")
        st.markdown("**Conclusion:** No statistically significant change in mean weight.")
        st.markdown("</div>", unsafe_allow_html=True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 5: t-Tests
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab5:
        st.markdown("<div class='section-card'><div class='section-label label-concept'>ğŸ’¡ t-Tests (Ïƒ Unknown)</div>", unsafe_allow_html=True)
        st.markdown("#### When to Use")
        st.markdown("""
- Population Ïƒ is **unknown** (the most common real-world case!)
- We estimate Ïƒ with the **sample standard deviation s**
- The t-distribution accounts for the extra uncertainty from estimating Ïƒ
- df = n âˆ’ 1 (for one-sample t-test)
        """)
        st.markdown("#### Test Statistic")
        st.latex(r"t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}} \sim t_{n-1}")
        st.markdown("#### Z-test vs t-test â€” When to Use Which?")
        st.markdown("""
| Scenario | Test | Why |
|----------|------|-----|
| Ïƒ known | **Z-test** | No extra uncertainty |
| Ïƒ unknown, n â‰¥ 30 | **t-test** (Z is okay too) | t â‰ˆ Z for large df |
| Ïƒ unknown, n < 30, pop ~Normal | **t-test** (required!) | Wider tails compensate for uncertainty |
| Ïƒ unknown, n < 30, pop non-Normal | âš ï¸ t-test approximate or use **nonparametric** | t less reliable |
        """)
        st.caption("In practice: when in doubt, use the t-test â€” it's always valid and converges to Z for large n.")
        st.markdown("</div>", unsafe_allow_html=True)

        st.markdown("<div class='section-card'><div class='section-label label-solved'>ğŸ§® Interactive t-Test Calculator</div>", unsafe_allow_html=True)
        col1, col2 = st.columns(2)
        with col1:
            mu_0t = st.number_input("Î¼â‚€:", value=75.0, step=1.0, key="tt_mu")
            xbar_t = st.number_input("xÌ„:", value=72.0, step=0.5, key="tt_xbar")
            s_t = st.number_input("s (sample SD):", value=8.0, min_value=0.01, step=0.5, key="tt_s")
            n_t = st.number_input("n:", value=16, min_value=2, key="tt_n")
            alpha_t = st.selectbox("Î±:", [0.01, 0.05, 0.10], index=1, key="tt_alpha")
            test_t = st.radio("Hâ‚:", ["Î¼ > Î¼â‚€", "Î¼ < Î¼â‚€", "Î¼ â‰  Î¼â‚€"], key="tt_type")
        with col2:
            df_t = n_t - 1
            se_t = s_t / np.sqrt(n_t)
            t_stat = (xbar_t - mu_0t) / se_t
            if ">" in test_t:
                pv_t = 1 - stats.t.cdf(t_stat, df_t)
                tc = stats.t.ppf(1 - alpha_t, df_t)
                rej_t = t_stat > tc
            elif "<" in test_t:
                pv_t = stats.t.cdf(t_stat, df_t)
                tc = stats.t.ppf(alpha_t, df_t)
                rej_t = t_stat < tc
            else:
                pv_t = 2 * (1 - stats.t.cdf(abs(t_stat), df_t))
                tc = stats.t.ppf(1 - alpha_t/2, df_t)
                rej_t = abs(t_stat) > tc
            st.metric("df", f"{df_t}")
            st.metric("SE = s/âˆšn", f"{se_t:.4f}")
            st.metric("t-statistic", f"{t_stat:.4f}")
            st.metric("p-value", f"{pv_t:.6f}")
            st.metric("Critical t*", f"Â±{tc:.3f}" if "â‰ " in test_t else f"{tc:.3f}")
            if rej_t:
                st.error(f"**REJECT Hâ‚€** at Î±={alpha_t} (p={pv_t:.4f} < {alpha_t})")
            else:
                st.success(f"**Fail to reject Hâ‚€** at Î±={alpha_t} (p={pv_t:.4f} â‰¥ {alpha_t})")
        st.markdown("</div>", unsafe_allow_html=True)

        st.markdown("<div class='section-card'><div class='section-label label-solved'>âœ… Solved Problems</div>", unsafe_allow_html=True)
        st.markdown("<span class='prob-badge'>Problem 1 â€” One-Sample t-Test</span>", unsafe_allow_html=True)
        st.markdown("""
**Q:** 16 students score xÌ„ = 72, s = 8 on a test. The expected mean is 75. At Î± = 0.05, is the class significantly below the expected mean?

**Solution:** Hâ‚€: Î¼ â‰¥ 75, Hâ‚: Î¼ < 75 (left-tailed), df = 15
        """)
        t_ex1 = (72-75)/(8/np.sqrt(16))
        pv_ex1 = stats.t.cdf(t_ex1, 15)
        tc_ex1 = stats.t.ppf(0.05, 15)
        st.latex(rf"t = \frac{{72-75}}{{8/\sqrt{{16}}}} = \frac{{-3}}{{2}} = {t_ex1:.3f}")
        st.latex(rf"t_{{0.05,15}} = {tc_ex1:.3f},\quad p = {pv_ex1:.4f}")
        st.markdown(f"Since |t| = 1.5 < |{tc_ex1:.3f}| and p = {pv_ex1:.4f} > 0.05: **Fail to reject Hâ‚€.** Insufficient evidence that the class is scoring below 75.")
        st.divider()

        st.markdown("<span class='prob-badge'>Problem 2 â€” t-Test for Quality Control</span>", unsafe_allow_html=True)
        st.markdown("""
**Q:** A coffee shop claims its large cup is 480ml. A consumer group samples 10 cups: xÌ„ = 471.2, s = 11.5. At Î± = 0.05, is the shop under-filling?

**Solution:** Hâ‚€: Î¼ â‰¥ 480, Hâ‚: Î¼ < 480 (left-tailed), df = 9
        """)
        t_ex2 = (471.2-480)/(11.5/np.sqrt(10))
        pv_ex2 = stats.t.cdf(t_ex2, 9)
        tc_ex2 = stats.t.ppf(0.05, 9)
        st.latex(rf"t = \frac{{471.2-480}}{{11.5/\sqrt{{10}}}} = \frac{{-8.8}}{{3.637}} = {t_ex2:.3f}")
        st.latex(rf"t_{{0.05,9}} = {tc_ex2:.3f},\quad p = {pv_ex2:.6f}")
        st.markdown(f"Since t = {t_ex2:.3f} < {tc_ex2:.3f} and p = {pv_ex2:.6f} < 0.05: **Reject Hâ‚€.** The shop is significantly under-filling its large cups.")
        st.markdown("</div>", unsafe_allow_html=True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TRICKY QUESTIONS (outside tabs)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    st.markdown("<div class='section-card'><div class='section-label label-tricky'>ğŸ§  Tricky Questions</div>", unsafe_allow_html=True)

    st.markdown("<span class='tricky-badge'>Tricky Q1</span>", unsafe_allow_html=True)
    st.markdown("**Q:** Can we *prove* Hâ‚€ is true by failing to reject it?")
    with st.expander("ğŸ” Reveal Solution"):
        st.markdown("""
**No â€” never.** Hypothesis testing can only reject Hâ‚€ or fail to reject it. Failing to reject means the data is *compatible* with Hâ‚€ â€” but it might also be compatible with many alternative values of Î¼.

**Analogy:** In a courtroom, "Not Guilty" does not mean "Innocent" â€” it means the prosecution didn't provide sufficient evidence.

To *support* Hâ‚€ (equivalence), you need a different framework: **equivalence testing** (TOST procedure), where you show Î¼ is within some practical margin of Î¼â‚€.
        """)

    st.markdown("<span class='tricky-badge'>Tricky Q2</span>", unsafe_allow_html=True)
    st.markdown("**Q:** A study with n=100,000 finds p=0.001 for a mean difference of 0.5 points (on a 100-point scale). Is this meaningful?")
    with st.expander("ğŸ” Reveal Solution"):
        st.markdown("""
**Statistically significant? Yes (p=0.001).**  
**Practically significant? No (0.5 points out of 100 is trivial).**

This illustrates the distinction between **statistical significance** and **practical significance**:

- With n = 100,000, SE is tiny, so even minuscule differences produce large test statistics and small p-values.
- **Effect size** (e.g. Cohen's d = 0.5/SD) would reveal the effect is negligible.
- Always report effect size, confidence intervals, AND p-values together.
        """)

    st.markdown("<span class='tricky-badge'>Tricky Q3</span>", unsafe_allow_html=True)
    st.markdown("**Q:** Researcher A tests at Î±=0.05 and gets p=0.04 â†’ rejects Hâ‚€. Researcher B tests the same data at Î±=0.01 â†’ p=0.04 â†’ fails to reject. Who is correct?")
    with st.expander("ğŸ” Reveal Solution"):
        st.markdown("""
**Both are correct.** They made different decisions because they chose different Î± levels â€” different tolerances for Type I error.

- Researcher A: 0.04 < 0.05 â†’ rejects (willing to accept 5% false positive rate)
- Researcher B: 0.04 > 0.01 â†’ fails to reject (requires 1% or less)

This is why **Î± must be declared before the test**, and why **reporting the exact p-value** is more informative than just "significant" or "not significant".
        """)
    st.markdown("</div>", unsafe_allow_html=True)
